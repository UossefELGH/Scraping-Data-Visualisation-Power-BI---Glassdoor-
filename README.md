# Scraping Data & Visualisation Power BI - Glassdoor Project  

## üìù Description  
Ce projet est divis√© en deux parties :  
1. **Data Scraping** : Extraction de donn√©es des offres d'emploi sur Glassdoor √† l'aide de Selenium et BeautifulSoup.
2. **Data Cleaning & Preparing** : Nettoyer et pr√©parer les donn√©es dans Excel pour l'analyse.  
3. **Power BI** : Analyse et visualisation des donn√©es extraites sous forme de tableau de bord interactif.  

---

## üöÄ Fonctionnalit√©s  
### Data Scraping  
- Extraction des informations sur les offres d'emploi (titre, entreprise, localisation, salaire, type de contrat).  
- Gestion des erreurs et pauses al√©atoires pour √©viter les d√©tections de bot.  
- R√©sultats enregistr√©s au format CSV.
- 
### MICROSOFT Excel 
-  Suppression des valeurs manquantes.
-  Ajouter de colonnes pertinentes.
-  Structuration des donn√©es pour l'analyse.  

### Power BI  
- Visualisation des tendances d'emploi (types de contrat, salaires, etc.).  
- Tableau de bord interactif pour explorer les donn√©es extraites.  

---

## üìÅ Structure des Dossiers  
- **`data/`** : Contient les donn√©es brutes et nettoy√©es.  
- **`scraping/`** : Scripts Python pour l'extraction des donn√©es.  
- **`powerbi/`** : Fichier Power BI et captures d'√©cran du tableau de bord.  
- **`docs/`** : Guides et documentations compl√©mentaires.  

---

## üõ†Ô∏è Installation et Utilisation  

### Pr√©-requis  
- Python 3.x
- Chromedriver 
- Selenium et BeautifulSoup
- Microsoft Excel
- Power BI Desktop  


---

## üõ†Ô∏è √âtapes du Projet  

### **1. Scraping des Donn√©es**  
**Outils et biblioth√®ques utilis√©s** :  
- `Python`, `Jupyter Notebook`  
- `Selenium`, `BeautifulSoup`, `Pandas`  
- `Chromedriver`  

**√âtapes suivies** :  
1. **Configuration de Selenium** :  
   - Utilisation de *Chromedriver* pour automatiser la navigation.  
   - Ajout d‚Äôun *user-agent* et d√©sactivation de la d√©tection d'automatisation.  

2. **Extraction des URL √† scraper** :  
   - G√©n√©ration des URLs dynamiquement pour diff√©rents intitul√©s de poste et types de contrat (CDI, CDD).  

3. **D√©veloppement du script de scraping** :  
   - Collecte des donn√©es suivantes pour chaque offre :  
     - **Titre du poste**  
     - **Nom de l'entreprise**  
     - **Localisation**  
     - **Salaire estim√©**  
     - **Type de contrat (CDI, CDD)**  

4. **Gestion des erreurs et pauses al√©atoires** :  
   - Ajout de pauses pour simuler une navigation humaine.  
   - Enregistrement des erreurs dans un fichier log (`scraping_errors.log`).  

5. **Enregistrement des donn√©es** :  
   - Sauvegarde des donn√©es dans un fichier CSV nomm√© `glassdoor_jobs_dataset.csv`.  

### Commandes pour ex√©cuter le script :  
```bash
# Installer les d√©pendances
pip install -r requirements.txt

# Ex√©cuter le script
python main.py
